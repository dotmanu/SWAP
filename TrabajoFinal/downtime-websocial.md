# *Downtime* en la web social o 2.0: estudiando su impacto socioeconómico

**Extracto**
 
> Un acercamiento al comportamiento social y económico de nuestro mundo en torno a la disponibilidad de los servicios de la web social o 2.0, entendiendo imprescindible su correcto funcionamiento para *1)* satisfacer nuestras necesidades digitales diarias y, *2)* permitir a empresas de todo tipo operar con normalidad, dada su inherente dependencia a sistemas informáticos en red.

## 1. Quiénes somos y a dónde vamos: la web social como un todo

Durante la celebración del Web 2.0 Summit 2010 en San Francisco, **Mark Zuckerberg** (fundador y director ejecutivo de *Facebook*) [explicaba](https://www.youtube.com/watch?v=Czw-dtTP6oU) cómo el diseño de una aplicación estaba condicionado por el componente social de la base de usuarios. “Ahora puedes diseñar algo con la idea de que la mayor parte de tus usuarios son propensos a socializar”, decía, vaticinando que la tendencia iría creciendo en los siguientes años. En 2017 podemos recuperar el concepto de web 2.0 e incluso, dejar de hablar de ello como una escisión de la web convencional: nuestros sistemas son totalmente sociales. Creamos webs y desarrollamos aplicaciones y servicios con un enfoque de usuario, preparados para la interoperabilidad, para intercambiar información.
 
Por tanto, hablar de web 2.0 o social en 2017 debe ser un ejercicio de comprensión; la realidad de una era tecnológica que nos ha llevado hasta aquí y que promete seguir evolucionando en favor del componente social. Cualquier estudio que hagamos sobre el funcionamiento de aplicaciones y servicios en red no puede perder de vista un objetivo claro: cualquier cambio, cualquier aspecto que involucre al usuario (o consumidor, pues no deja de ser una persona que consume o utiliza un servicio), tendrá una consecuencia directa en la evolución de la aplicación. Será siempre el conjunto de usuarios que utiliza una aplicación el que dictamine si esa aplicación sobrevive o queda relegada al olvido.
 
Ya sea en el caso de las redes sociales, en constante batalla por la cuota de usuarios (*Facebook*, *Twitter* o *Instagram* como ejemplos de éxito permanente en contrapartida a *Snapchat* o *Uber*, casos recientes de grandes empresas tecnológicas que han sufrido la [pérdida de usuarios](https://techcrunch.com/2017/01/30/attack-of-the-clone/) o una [puesta en duda](http://fortune.com/2017/02/03/uber-lyft-delete-donald-trump-executive-order/) del servicio), como en servicios a empresas (proveedores de servidores, aplicaciones de gestión, etcétera) o simples webs de comercio online, la necesidad de estar en constante conexión con el usuario, se torna imprescindible.

## 2. El gran problema de nuestros días: *downtime* o tiempo de inactividad

Que un servicio no esté disponible, en un mundo completamente dependiente de las tecnologías, se traduce directamente en un varapalo para la empresa proveedora del servicio. Hablar de downtime o tiempo de actividad es hablar, toscamente, de la caída de un servidor y cómo esto afecta al funcionamiento de una aplicación dependiente de la red para su funcionamiento. Si una empresa no es capaz de responder ante una caída de los servidores con la suficiente rapidez e inteligencia (con una buena comunicación, aplicando compensaciones cuando corresponda, entre otras estrategias), la confianza del cliente se puede ver afectada gravemente. Es por ello que representa un problema de total actualidad, afectando inevitablemente a la relación entre consumidor y proveedor del servicio. 

Veamos dos ejemplos básicos. Por una parte, aplicaciones enfocadas a los negocios (CRM, ERP), sin las que una empresa no podría acceder a su información, comprometiendo las operaciones de dicha organización, imposibilitando la integración y parando el flujo de datos entre las distintos niveles de la empresa. Por otra, servicios tecnológicos, lo básico de los básico. Nuestra dependencia ante el acceso a nuestro email, internet o servicios de intranet en empresas, nos hace susceptibles del colapso ante la caída de servidores. 

### 2.1. Por qué ocurre: causas del *downtime*

Según un estudio del *Penomon Institute* realizado a finales del año pasado, en porcentaje, la mayor parte de los problemas relacionados con el tiempo de actividad se derivan de errores que no se pueden controlar: fallos en los sistemas de alimentación ininterrumpida, causas relacionadas con la temperatura, el clima, fallos en el generador... Otra buena parte del *downtime* es debido a errores accidentales, esto es, a errores humanos. 

![alt text](http://i.imgur.com/NcWJtXF.png)

Como se puede observar en la gráfica, si bien durante los últimos años se han mantenido los porcentajes relativos a las diferentes causas, los ciberataques (especialmente aquellos de denegación de servicio, DDoS) se han multiplicado, pasando de apenas un 2% en 2010 a un 22% del total en 2016; su aumento en los próximos años está garantizado. Este es el problema más significativo a día de hoy y el más difícil de combatir.

### 2.2. Consecuencias de una mala gestión

La consecuencia principal del *downtime* es la productividad. Que una empresa no pueda acceder a sus herramientas básicas de operación llevará a una pérdida de tiempo, lo que se traduce directamente en una pérdida de dinero. Aunque el tiempo de inactividad puede suponer pérdidas económicas puntuales, no abordar el problema correctamente puede llevar a un auténtico descalabro para una empresa.
 
Pérdida de oportunidades (transacciones, compras online, acuerdos), pérdida de confianza por parte de otros clientes (desembocando en la ruptura de relaciones comerciales), penalizaciones debido a la falta de disponibilidad... Todo ello podría resultar en una caída en la reputación y, por ende, en la condena pública a una empresa a la pérdida total de su base de usuarios.

## 3. El coste económico: el precio del tiempo

Si nos vamos a datos de 2013, según el *Ponemon Institute*, el coste por minuto de tiempo de inactividad rozaba los ocho mil dólares, superando en más de un 40% a la cifra de 2010. Teniendo en cuenta que en 2013 la media de *downtime* por empresa fue de 83 minutos, estaríamos hablando de un coste de 690.200 dólares. Si bien se trata de una cifra estimada, no deja de ser representativo. Atendiendo a los datos de 2016, la cifra asciende hasta los 740.350 dólares. Claro está, los datos difieren totalmente entre unas empresas y otras: el año pasado, la cifra más alta de gasto derivado del *downtime* en una empresa es de 2.4 millónes de dólares, un aumento del 38'95% con respecto a 2013 y de un 136'8% con respecto a 2010. 

![alt text](http://i.imgur.com/sJWuQlw.png)

Si atendemos a su clasificación por tipo de gasto derivado del *downtime*, podemos extraer información interesante. Por un lado, hay que remarcar el hecho de que los costes indirectos sean superiores a los directos. Esto implica que solucionar un problema (es decir, acotar un problema a un determinado coste) no solo no cierra la puerta a otros costes, sino que estos lo superan. Además, podemos ver como en menor medida, el coste de oportunidad aparece asociado a algunas empresas. Esto es, "el valor de la mejor opción no realizada". El coste de oportunidad tiene que ver con recursos limitados y las oportunidades disponibles, basándose en el principio de la rentabilidad esperada: tomar decisiones en base al coste o privación de recursos para conseguir la mayor rentabilidad. ¿Qué pinta aquí, hablando de servidores? Pinta bastante: se trata del costo resultante de la pérdida de oportunidades de negocio como consecuencia de la disminución de la reputación después de la caída del servicio. Volvemos a uno de los aspectos clave: no solo se trata del coste concreto que conlleva un tiempo de inactividad determinado, sino sus gastos asociados; los indirectos y aquellos derivados de la pérdida de confianza en el servicio.

![alt text](http://i.imgur.com/8USkqcN.png)

En el siguiente gráfico, finalmente, vemos la relación entre la cuantía económica y el paso del tiempo en un período de *downtime*, de donde podemos extraer (a juzgar por la recta de regresión) que, efectivamente, cuanto más tiempo se tarda en actuar, más asciende el coste económico de la caída de un servidor. 

![alt text](http://i.imgur.com/xf96xV7.png)

Tal y como reza la frase usualmente acreditada a Benjamin Franklin, *time is money*, el tiempo es dinero. La alta disponibilidad en relación al funcionamiento continuo de los sistemas informáticos en caso de una interrupción es clave para evitar pérdidas económicas. 

## 4. Pérdida de confianza: un reflejo social

Llegados a este punto es donde realmente entre en juego la web social o 2.0, si bien ya se ha explicado que, dada su extensibilidad a todos los campos de la sociedad actual, podríamos obviar incidir en el término como algo no ligado al estudio. Sin embargo, conviene incidir en esta parte en el papel que juega la disponibilidad de los servicios en una rama muy concreta de esa web 2.0: las redes sociales.

La caída de servicios tan fundamentales en nuestra vida diaria para la comunicación como Gmail, o a los que nos hemos hecho totalmente dependientes para mensajearnos como Twitter, Facebook o WhatsApp, puede provocar un verdadero impacto a escala social. Aunque no existen datos estadísticos sobre los efectos del tiempo de inactividad experimentado por estas aplicaciones, podemos mirar atrás y estudiar los datos de algunas de sus caídas más relevantes.

### Twitter y su ballena azul: caídas constantes en sus primeros años

En los primeros años de Twitter, la red social se hizo bien conocida, entre otras cosas, por su página de error. Se trataba de una ballena a la que transportaban varios pájaros, emulando el logo de Twitter. El mensaje "Twitter is over capacity" era habitual mientras la red social crecía: actualizaciones, nuevas características... Twitter mejoraba su infraestructura, pero las caídas eran inevitables: la base de usuarios no dejaba de crecer. La CNN titulaba en 2010: "Twitter can't kill the Fail Whale". Las caídas eran recurrentes: de hasta una hora de duración, a veces más, y una comunidad de usuarios que, si bien se fue acostumbrando, no pudo evitar perder parte del interés en Twitter: algunos usuarios encontraron en Tumblr una alternativa a las continuas caídas del servidor.

El 26 de junio de 2012, Twitter pedía disculpas por la caída del servicio durante 40 minutos a nivel mundial. La aplicación había empezado a fallar a las 8:20 de la mañana y no sería hasta las 10:25 cuando Twitter podía navegarse con normalidad de nuevo. ¿La causa? Su data center, diseñado (como es lógico) para ser redundante, había experimentado el fallo de dos sistemas paralelos prácticamente al mismo tiempo, imposibilitando esa redundancia. La red social anunció medidas para evitar la situación en el futuro. En los años siguientes, Twitter ha experimentado pequeñas caídas que siempre se saldan con cierta desesperación por parte de sus usuarios, consumida una vez el servicio vuelve a su funcionamiento.

### Gmail y Google+ se caen en 2014, afectando a millones de usuarios

Gmail, Google+ y otros servicios de Google como Hangouts o Drive dejaban de funcionar en junio de 2014. Al caerse Google+, afectaba también a Youtube, cuyo sistema de comentarios funcionaba a través de dicha plataforma. El buscador pareció verse afectado durante unos pocos minutos, también. Un acontecimiento que afectó a millones de usuarios, superando la gran caída de los servicios de Google en 2013, los cuales afectaron a un 50% de los usuarios.

La dependencia de algunos negocios hacia Google, tanto para ser accedidos a través de sus búsquedas como para utilizar servicios como Gmail, hacen de una eventual caída de Google una de las grandes preocupaciones del sector tecnológico. En cuanto a sus usuarios, tiene un punto divertido: en esta ocasión, el descontrol y las quejas se virtieron, fundamentalmente, a través de Twitter. No es la primera vez: cuando se cae una red social, otra adquiere protagonismo para servir de salvoconducto a la caída del servicio.

### WhatsApp: caída total en Nochevieja de 2015 y auge de Telegram

El 31 de diciembre de 2015, WhatsApp se caía. Los servidores fallaban en la noche más importante del año: felicitaciones, planes y conversaciones de todo tipo, frustradas. La app de mensajería más famosa del mundo no estaba ofreciendo el servicio esperado en un día crucial, y sus usuarios abrazaron la frustración; la dependencia hacia WhatsApp les estaba complicando la vida.

Efectos directos: al día siguiente, Telegram, otra aplicación de mensajería, conseguía aumentar su base de usuarios en ni más ni menos que 5 millones. Una auténtica barbaridad que le costó a Telegram sus respectivas dos horas de downtime durante el primer día de 2016. Telegram no estaba preparada para tal cantidad de nuevos registros y, aunque añadieron nuevos servidores, fue imposible dar respuesta al reguero constante de usuarios.

## 5. Un caso de buena praxis: cómo Facebook transfirió todo el contenido de Instagram a sus servidores

**Antecedentes:** antes de su compra por parte de Facebook, Instagram contaba con un servidor operado por Ubuntu Linux 11.04 y almacenado en Amazon EC2. Para el balanceador de carga, empezaron utilizando 2 máquinas con nginx y un DNS de tipo Round Robin entre ellas; más tarde, se pasaron al Elastic Load Balancer de Amazon, con tres instancias nginx intercambiables. Para el DNS también utilizaron un servicio de Amazon, concretamente el Route53. ¿Servidor de aplicaciones? Instagram tenía Django funcionando en las máquinas Amazon High-CPU Extra-Large. Como protocolo de transmisión del Web Server Gateway Interface (WSGI), [Gunicorn](http://gunicorn.org), un servidor HTTP para UNIX basado en Python. Para ejecutar comandos, [Fabric](http://www.fabfile.org).
 
Por último, pero no menos importante, el almacenamiento de datos. Instagram utilizaba PostgreSQL. Todas sus instancias de PostgreSQL se ejecutaban en una configuración de réplica utilizando SR (Streaming Replication) y valiéndose de Elastic Load Balancer para realizar copias de seguridad de sus sistemas. Para almacenar las fotos, directas a Amazon S3 (Amazon Simple Storage Service), donde había varios terabytes de datos. Como CDN, Amazon CloudFront, con la idea de mejorar los tiempos de carga alrededor del globo. [Memcached](https://memcached.org) para la caché. Apache Solr para su API de búsqueda geolocalizada con interfaz en JSON. Como estructura de almacenamiento de datos (para el feed de imágenes, de actividad e sistema de sesiones), [Redis](https://redis.io).

**La transferencia**: de Amazon, al data center propio de Facebook. 20 mil millones de fotos a transferir, 200 millones de usuarios que no se enteraron del cambio de servidores y que pudieron seguir utilizando Instagram con total normalidad; un ejemplo de buena praxis en la forma de proceder. En tiempo, un año completo de trabajo de un pequeño equipo encargado de que todo saliera bien. ¿Primer paso? Hacer una copia del software que hacía funcionar Instagram en Amazon, de ahí la importancia de conocer en qué consistía. Una vez hecho, se pudo empezar a trabajar en la transferencia de los datos. 

El proceso consistió en dos transferencias. La idea era trasladar los datos al Virtual Private Cloud de Amazon (VPC) una herramienta que permitiría crear una red lógica que conectara a Amazon con el centro de datos de Facebook. Crear esta red era importante, ya que daba a Facebook control total sobre las direcciones de red utilizadas por las máquinas que ejecutaban Instagram en Amazon, permitiendo definir sus propias direcciones y evitando miles de conflictos de direcciones al pasar el software al data center de Facebook.

Cuando Instagram fue fundado en 2010, el servicio VPC de Amazon no existía, lo cual habría ahorrado tiempo en la transferencia (de hecho, el propio equipo encargado de la transferencia recomienda VPC como punto de partida para una *startup*). Sin embargo, antes de llegar al punto del VPC, fue necesario crear una red común entre el VPC y el EC2, donde estaba alojado Instagram originalmente. Para ello, Facebook desarrolló una herramienta de red propia llamada *Neti*. 

Una vez todo estaba listo para mover el software, se utilizó una herramienta llamada [Chef](https://www.chef.io/chef/), la que permite escribir “recetas” automatizadas para cargar y configurar las máquinas del data center encargadas de hacer funcionar Instagram. Con estas “recetas” pudieron cargar el software original que utilizaba Instagram en el data center de Facebook: base de datos, almacenamiento en caché, etcétera.
 
A día de hoy, Instagram se ejecuta en las máquinas dedicadas del data center de Facebook. Según los ingenieros de Facebook, ahora Instagram funciona con más eficiencia: utiliza un servidor por cada tres que usaba en Amazon y han mejorado los tiempos de recolección de datos en un 80%. Con esta “fusión”, además, permiten a Instagram conectarse con más rapidez a otros servicios de Facebook, facilitando las tareas de *big data* y de lucha contra el *spam*.

## 6. Conclusiones: prevenir y actuar contra el *downtime*

Tras repasar los distintos aspectos que están involucrados en el tiempo de inactividad de los servicios a los que accedemos diariamente, llegamos a la conclusión de la necesidad de adoptar medidas que permitan reducir el coste y el impacto de posibles caídas. Para ello, debemos contar con un equipo capaz de tomar decisiones y adoptar prioridades en función del coste del *downtime* (el cual ha ido creciendo en los últimos años). Además, es necesario llevar un control exhaustivo y constante de nuestro soporte técnico: personas involucradas, procesos y sobre todo, la tecnología con la que se llevan a cabo. Para ello será necesario que el equipo encargado del soporte técnico sea parte fundamental de la empresa, servicio, aplicación; el hecho de que funcione en paralelo con respecto a la infraestructura permitirá una mejor respuesta ante situaciones complicadas.
 
Teniendo en cuenta estos factores, se debe: contar con una estrategia de respuesta clara que aborde todos los niveles del equipo; tomar decisiones que aboguen por la calidad del servicio y no por reducir costes que a posteriori pueden incrementarse exponencialmente por no haber respondido con contundencia; estar al tanto de metodologías, herramientas, logística y capital intelectual. Por último, pero no menos importante, debemos ser críticos con nuestro proveedor. Si nuestro servicio depende de un agente externo para funcionar (es decir, si dependemos de sus servidores), debemos evaluar la calidad del soporte. Las veces que la respuesta ante un problema ha sido positiva y hemos solucionado nuestra problemática a la primera, el tiempo que ha llevado solucionarse y los distintos factores implicados en su solución.
 
En definitiva, podemos decir que viviendo en el momento histórico en el que vivimos, la disponibilidad de nuestros servicios como empresa debe ser primordial. No podemos traicionar la confianza del usuario y, mucho menos, poner en duda la calidad de nuestro servicio. Teniendo en cuenta la brutal competencia que hay ahí fuera, es primordial reducir nuestro downtime o tiempo de respuesta al mínimo. Por una parte, porque puede afectar negativamente a nuestros ingresos y, por otro, porque puede minar la confianza del público en nuestro producto, llevándonos a una pérdida de usuarios y al fin total de nuestra aventura tecnológica.

## Bibliografía

**[1]** IBM Global Technology Services. 2014. [Time is money and downtime is expensive](http://public.dhe.ibm.com/common/ssi/ecm/mt/en/mtw03011usen/MTW03011USEN.PDF?cm_mc_uid=40722369935914958827176&cm_mc_sid_50200000=1495882717&cm_mc_sid_52640000=1495882717). 

**[2]** Ponemon Institute. 2016. [Cost of Data Center Outages](http://planetaklimata.com.ua/instr/Liebert_Hiross/Cost_of_Data_Center_Outages_2016_Eng.pdf).

**[3]** McClary, Rob. Data Center Journal. 2015. [Unplanned Downtime: What does it cost your business](http://www.datacenterjournal.com/unplanned-downtime-cost-business/).

**[4]** Carl Villanueva, John. JEscape. 2015. [5 Major Consequences of Downtime](http://www.jscape.com/blog/consequences-of-downtime).

**[5]** Metz, Cade. TechRadar. 2014. [How Facebook Moved 20 Billion Instagram Photos Without You Noticing](https://www.wired.com/2014/06/facebook-instagram/).

**[6]** Segall, Laurie. CNN Money. 2010. [Twitter can't kill the Fail Whale](http://money.cnn.com/2010/06/15/technology/twitter_fail/index.htm?iid=EL).

**[7]** Rawashdeh, Mazen. Twitter Inc. 2012. [Our apologies for today’s outage](https://blog.twitter.com/official/en_us/a/2012/our-apologies-for-today-s-outage.html).

**[8]** Clay, Kelly. Forber. 2014. [Gmail And Google+ Go Down On Friday, Impacting Millions Of Users](https://www.forbes.com/sites/kellyclay/2014/01/24/gmail-and-google-go-down-on-friday-impacting-millions-of-users/#3b1849d46568).

**[9]** Dredge, Stuart. The Guardian. 2014. [Messaging app Telegram added 5m new users the day after WhatsApp outage](https://www.theguardian.com/technology/2014/feb/24/telegram-messaging-app-whatsapp-down-facebook).

**[10]** Instagram Engineering. Medium. 2011. [What Powers Instagram: Hundreds of Instances, Dozens of Technologies](https://engineering.instagram.com/what-powers-instagram-hundreds-of-instances-dozens-of-technologies-adf2e22da2ad).
